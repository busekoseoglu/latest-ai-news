{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45eb3404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import ssl\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# SSL sertifika doğrulama sorununu çözmek için\n",
    "if hasattr(ssl, '_create_unverified_context'):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6de6a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [\"https://bair.berkeley.edu/blog/feed.xml\",\n",
    "        \"https://feeds.feedburner.com/nvidiablog\",\n",
    "        \"https://www.oreilly.com/radar/feed/\",\n",
    "        \"https://www.microsoft.com/en-us/research/feed/\",\n",
    "        \"https://www.sciencedaily.com/rss/computers_math/artificial_intelligence.xml\",\n",
    "        \"https://research.facebook.com/feed/\",\n",
    "        \"https://openai.com/news/rss.xml\",\n",
    "        \"https://deepmind.google/blog/feed/basic/\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41917a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Run Llama 2 uncensored locally',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Run Llama 2 uncensored locally'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/run-llama2-uncensored-locally'}],\n",
       "  'link': 'https://ollama.com/blog/run-llama2-uncensored-locally',\n",
       "  'summary': 'This post will give some example comparisons running Llama 2 uncensored model versus its censored model.',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'This post will give some example comparisons running Llama 2 uncensored model versus its censored model.'},\n",
       "  'id': 'https://ollama.com/blog/run-llama2-uncensored-locally',\n",
       "  'guidislink': False,\n",
       "  'published': 'Tue, 01 Aug 2023 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=8, tm_mday=1, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=1, tm_yday=213, tm_isdst=0)},\n",
       " {'title': 'Run Code Llama locally',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Run Code Llama locally'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/run-code-llama-locally'}],\n",
       "  'link': 'https://ollama.com/blog/run-code-llama-locally',\n",
       "  'summary': \"Meta's Code Llama is now available on Ollama to try.\",\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': \"Meta's Code Llama is now available on Ollama to try.\"},\n",
       "  'id': 'https://ollama.com/blog/run-code-llama-locally',\n",
       "  'guidislink': False,\n",
       "  'published': 'Thu, 24 Aug 2023 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=8, tm_mday=24, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=3, tm_yday=236, tm_isdst=0)},\n",
       " {'title': 'How to prompt Code Llama',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'How to prompt Code Llama'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/how-to-prompt-code-llama'}],\n",
       "  'link': 'https://ollama.com/blog/how-to-prompt-code-llama',\n",
       "  'summary': 'This guide walks through the different ways to structure prompts for Code Llama and its different variations and features including instructions, code completion and fill-in-the-middle (FIM).',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'This guide walks through the different ways to structure prompts for Code Llama and its different variations and features including instructions, code completion and fill-in-the-middle (FIM).'},\n",
       "  'id': 'https://ollama.com/blog/how-to-prompt-code-llama',\n",
       "  'guidislink': False,\n",
       "  'published': 'Sat, 09 Sep 2023 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=9, tm_mday=9, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=5, tm_yday=252, tm_isdst=0)},\n",
       " {'title': 'Leveraging LLMs in your Obsidian Notes',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Leveraging LLMs in your Obsidian Notes'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/llms-in-obsidian'}],\n",
       "  'link': 'https://ollama.com/blog/llms-in-obsidian',\n",
       "  'summary': 'This post walks through how you could incorporate a local LLM using Ollama in Obsidian, or potentially any note taking tool.',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'This post walks through how you could incorporate a local LLM using Ollama in Obsidian, or potentially any note taking tool.'},\n",
       "  'id': 'https://ollama.com/blog/llms-in-obsidian',\n",
       "  'guidislink': False,\n",
       "  'published': 'Thu, 21 Sep 2023 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=9, tm_mday=21, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=3, tm_yday=264, tm_isdst=0)},\n",
       " {'title': 'Ollama is now available as an official Docker image',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Ollama is now available as an official Docker image'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image'}],\n",
       "  'link': 'https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image',\n",
       "  'summary': 'Ollama can now run with Docker Desktop on the Mac, and run inside Docker containers with GPU acceleration on Linux.',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Ollama can now run with Docker Desktop on the Mac, and run inside Docker containers with GPU acceleration on Linux.'},\n",
       "  'id': 'https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image',\n",
       "  'guidislink': False,\n",
       "  'published': 'Thu, 05 Oct 2023 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=10, tm_mday=5, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=3, tm_yday=278, tm_isdst=0)},\n",
       " {'title': 'Building LLM-Powered Web Apps with Client-Side Technology',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Building LLM-Powered Web Apps with Client-Side Technology'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/building-llm-powered-web-apps'}],\n",
       "  'link': 'https://ollama.com/blog/building-llm-powered-web-apps',\n",
       "  'summary': 'Recreate one of the most popular LangChain use-cases with open source, locally running software - a chain that performs Retrieval-Augmented Generation, or RAG for short, and allows you to “chat with your documents”',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Recreate one of the most popular LangChain use-cases with open source, locally running software - a chain that performs Retrieval-Augmented Generation, or RAG for short, and allows you to “chat with your documents”'},\n",
       "  'id': 'https://ollama.com/blog/building-llm-powered-web-apps',\n",
       "  'guidislink': False,\n",
       "  'published': 'Fri, 13 Oct 2023 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=10, tm_mday=13, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=286, tm_isdst=0)},\n",
       " {'title': 'Python & JavaScript Libraries',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Python & JavaScript Libraries'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/python-javascript-libraries'}],\n",
       "  'link': 'https://ollama.com/blog/python-javascript-libraries',\n",
       "  'summary': 'The initial versions of the Ollama Python and JavaScript libraries are now available, making it easy to integrate your Python or JavaScript, or Typescript app with Ollama in a few lines of code. Both libraries include all the features of the Ollama REST API, are familiar in design, and compatible with new and previous versions of Ollama.',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'The initial versions of the Ollama Python and JavaScript libraries are now available, making it easy to integrate your Python or JavaScript, or Typescript app with Ollama in a few lines of code. Both libraries include all the features of the Ollama REST API, are familiar in design, and compatible with new and previous versions of Ollama.'},\n",
       "  'id': 'https://ollama.com/blog/python-javascript-libraries',\n",
       "  'guidislink': False,\n",
       "  'published': 'Tue, 23 Jan 2024 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2024, tm_mon=1, tm_mday=23, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=1, tm_yday=23, tm_isdst=0)},\n",
       " {'title': 'Vision models',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Vision models'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/vision-models'}],\n",
       "  'link': 'https://ollama.com/blog/vision-models',\n",
       "  'summary': 'New vision models are now available: LLaVA 1.6, in 7B, 13B and 34B parameter sizes. These models support higher resolution images, improved text recognition and logical reasoning.',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'New vision models are now available: LLaVA 1.6, in 7B, 13B and 34B parameter sizes. These models support higher resolution images, improved text recognition and logical reasoning.'},\n",
       "  'id': 'https://ollama.com/blog/vision-models',\n",
       "  'guidislink': False,\n",
       "  'published': 'Fri, 02 Feb 2024 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2024, tm_mon=2, tm_mday=2, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=33, tm_isdst=0)},\n",
       " {'title': 'OpenAI compatibility',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'OpenAI compatibility'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/openai-compatibility'}],\n",
       "  'link': 'https://ollama.com/blog/openai-compatibility',\n",
       "  'summary': 'Ollama now has initial compatibility with the OpenAI Chat Completions API, making it possible to use existing tooling built for OpenAI with local models via Ollama.',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Ollama now has initial compatibility with the OpenAI Chat Completions API, making it possible to use existing tooling built for OpenAI with local models via Ollama.'},\n",
       "  'id': 'https://ollama.com/blog/openai-compatibility',\n",
       "  'guidislink': False,\n",
       "  'published': 'Thu, 08 Feb 2024 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2024, tm_mon=2, tm_mday=8, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=3, tm_yday=39, tm_isdst=0)},\n",
       " {'title': 'Windows preview',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Windows preview'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/windows-preview'}],\n",
       "  'link': 'https://ollama.com/blog/windows-preview',\n",
       "  'summary': 'Ollama is now available on Windows in preview, making it possible to pull, run and create large language models in a new native Windows experience. Ollama on Windows includes built-in GPU acceleration, access to the full model library, and serves the Ollama API including OpenAI compatibility.',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Ollama is now available on Windows in preview, making it possible to pull, run and create large language models in a new native Windows experience. Ollama on Windows includes built-in GPU acceleration, access to the full model library, and serves the Ollama API including OpenAI compatibility.'},\n",
       "  'id': 'https://ollama.com/blog/windows-preview',\n",
       "  'guidislink': False,\n",
       "  'published': 'Thu, 15 Feb 2024 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2024, tm_mon=2, tm_mday=15, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=3, tm_yday=46, tm_isdst=0)},\n",
       " {'title': 'Ollama now supports AMD graphics cards',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Ollama now supports AMD graphics cards'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/amd-preview'}],\n",
       "  'link': 'https://ollama.com/blog/amd-preview',\n",
       "  'summary': 'Ollama now supports AMD graphics cards in preview on Windows and Linux. All the features of Ollama can now be accelerated by AMD graphics cards on Ollama for Linux and Windows.',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Ollama now supports AMD graphics cards in preview on Windows and Linux. All the features of Ollama can now be accelerated by AMD graphics cards on Ollama for Linux and Windows.'},\n",
       "  'id': 'https://ollama.com/blog/amd-preview',\n",
       "  'guidislink': False,\n",
       "  'published': 'Thu, 14 Mar 2024 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2024, tm_mon=3, tm_mday=14, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=3, tm_yday=74, tm_isdst=0)},\n",
       " {'title': 'Embedding models',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Embedding models'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/embedding-models'}],\n",
       "  'link': 'https://ollama.com/blog/embedding-models',\n",
       "  'summary': 'Embedding models are available in Ollama, making it easy to generate vector embeddings for use in search and retrieval augmented generation (RAG) applications.',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Embedding models are available in Ollama, making it easy to generate vector embeddings for use in search and retrieval augmented generation (RAG) applications.'},\n",
       "  'id': 'https://ollama.com/blog/embedding-models',\n",
       "  'guidislink': False,\n",
       "  'published': 'Mon, 08 Apr 2024 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2024, tm_mon=4, tm_mday=8, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=0, tm_yday=99, tm_isdst=0)},\n",
       " {'title': 'Llama 3',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Llama 3'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/llama3'}],\n",
       "  'link': 'https://ollama.com/blog/llama3',\n",
       "  'summary': \"Llama 3 is now available to run on Ollama. This model is the next generation of Meta's state-of-the-art large language model, and is the most capable openly available LLM to date.\",\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': \"Llama 3 is now available to run on Ollama. This model is the next generation of Meta's state-of-the-art large language model, and is the most capable openly available LLM to date.\"},\n",
       "  'id': 'https://ollama.com/blog/llama3',\n",
       "  'guidislink': False,\n",
       "  'published': 'Thu, 18 Apr 2024 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2024, tm_mon=4, tm_mday=18, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=3, tm_yday=109, tm_isdst=0)},\n",
       " {'title': 'Llama 3 is not very censored',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Llama 3 is not very censored'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/llama-3-is-not-very-censored'}],\n",
       "  'link': 'https://ollama.com/blog/llama-3-is-not-very-censored',\n",
       "  'summary': 'Compared to Llama 2, Llama 3 feels much less censored. Meta has substantially lowered false refusal rates. Llama 3 will refuse less than 1/3 of the prompts previously refused by Llama 2.',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Compared to Llama 2, Llama 3 feels much less censored. Meta has substantially lowered false refusal rates. Llama 3 will refuse less than 1/3 of the prompts previously refused by Llama 2.'},\n",
       "  'id': 'https://ollama.com/blog/llama-3-is-not-very-censored',\n",
       "  'guidislink': False,\n",
       "  'published': 'Fri, 19 Apr 2024 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2024, tm_mon=4, tm_mday=19, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=110, tm_isdst=0)},\n",
       " {'title': 'Google announces Firebase Genkit with Ollama support',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Google announces Firebase Genkit with Ollama support'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/firebase-genkit'}],\n",
       "  'link': 'https://ollama.com/blog/firebase-genkit',\n",
       "  'summary': 'At Google IO 2024, Google announced Ollama support in Firebase Genkit, a new open-source framework for developers to build, deploy and monitor production-ready AI-powered apps.',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'At Google IO 2024, Google announced Ollama support in Firebase Genkit, a new open-source framework for developers to build, deploy and monitor production-ready AI-powered apps.'},\n",
       "  'id': 'https://ollama.com/blog/firebase-genkit',\n",
       "  'guidislink': False,\n",
       "  'published': 'Mon, 20 May 2024 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2024, tm_mon=5, tm_mday=20, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=0, tm_yday=141, tm_isdst=0)},\n",
       " {'title': 'An entirely open-source AI code assistant inside your editor',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'An entirely open-source AI code assistant inside your editor'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/continue-code-assistant'}],\n",
       "  'link': 'https://ollama.com/blog/continue-code-assistant',\n",
       "  'summary': 'Continue enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs.',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Continue enables you to easily create your own coding assistant directly inside Visual Studio Code and JetBrains with open-source LLMs.'},\n",
       "  'id': 'https://ollama.com/blog/continue-code-assistant',\n",
       "  'guidislink': False,\n",
       "  'published': 'Fri, 31 May 2024 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2024, tm_mon=5, tm_mday=31, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=152, tm_isdst=0)},\n",
       " {'title': 'Google Gemma 2',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Google Gemma 2'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/gemma2'}],\n",
       "  'link': 'https://ollama.com/blog/gemma2',\n",
       "  'summary': 'Gemma 2 is now available on Ollama in 3 sizes - 2B, 9B and 27B.',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Gemma 2 is now available on Ollama in 3 sizes - 2B, 9B and 27B.'},\n",
       "  'id': 'https://ollama.com/blog/gemma2',\n",
       "  'guidislink': False,\n",
       "  'published': 'Thu, 27 Jun 2024 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2024, tm_mon=6, tm_mday=27, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=3, tm_yday=179, tm_isdst=0)},\n",
       " {'title': 'Tool support',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Tool support'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/tool-support'}],\n",
       "  'link': 'https://ollama.com/blog/tool-support',\n",
       "  'summary': 'Ollama now supports tool calling with popular models such as Llama 3.1. This enables a model to answer a given prompt using tool(s) it knows about, making it possible for models to perform more complex tasks or interact with the outside world.',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Ollama now supports tool calling with popular models such as Llama 3.1. This enables a model to answer a given prompt using tool(s) it knows about, making it possible for models to perform more complex tasks or interact with the outside world.'},\n",
       "  'id': 'https://ollama.com/blog/tool-support',\n",
       "  'guidislink': False,\n",
       "  'published': 'Thu, 25 Jul 2024 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2024, tm_mon=7, tm_mday=25, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=3, tm_yday=207, tm_isdst=0)},\n",
       " {'title': 'Reduce hallucinations with Bespoke-Minicheck',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Reduce hallucinations with Bespoke-Minicheck'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/reduce-hallucinations-with-bespoke-minicheck'}],\n",
       "  'link': 'https://ollama.com/blog/reduce-hallucinations-with-bespoke-minicheck',\n",
       "  'summary': 'Bespoke-Minicheck is a new grounded factuality checking model developed by Bespoke Labs that is now available in Ollama. It can fact-check responses generated by other models to detect and reduce hallucinations.',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Bespoke-Minicheck is a new grounded factuality checking model developed by Bespoke Labs that is now available in Ollama. It can fact-check responses generated by other models to detect and reduce hallucinations.'},\n",
       "  'id': 'https://ollama.com/blog/reduce-hallucinations-with-bespoke-minicheck',\n",
       "  'guidislink': False,\n",
       "  'published': 'Wed, 18 Sep 2024 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2024, tm_mon=9, tm_mday=18, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=2, tm_yday=262, tm_isdst=0)},\n",
       " {'title': 'Llama 3.2 goes small and multimodal',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Llama 3.2 goes small and multimodal'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/llama3.2'}],\n",
       "  'link': 'https://ollama.com/blog/llama3.2',\n",
       "  'summary': 'Ollama partners with Meta to bring Llama 3.2 to Ollama.',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Ollama partners with Meta to bring Llama 3.2 to Ollama.'},\n",
       "  'id': 'https://ollama.com/blog/llama3.2',\n",
       "  'guidislink': False,\n",
       "  'published': 'Wed, 25 Sep 2024 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2024, tm_mon=9, tm_mday=25, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=2, tm_yday=269, tm_isdst=0)},\n",
       " {'title': 'IBM Granite 3.0 models',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'IBM Granite 3.0 models'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/ibm-granite'}],\n",
       "  'link': 'https://ollama.com/blog/ibm-granite',\n",
       "  'summary': 'Ollama partners with IBM to bring Granite 3.0 models to Ollama.',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Ollama partners with IBM to bring Granite 3.0 models to Ollama.'},\n",
       "  'id': 'https://ollama.com/blog/ibm-granite',\n",
       "  'guidislink': False,\n",
       "  'published': 'Mon, 21 Oct 2024 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2024, tm_mon=10, tm_mday=21, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=0, tm_yday=295, tm_isdst=0)},\n",
       " {'title': 'Llama 3.2 Vision',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Llama 3.2 Vision'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/llama3.2-vision'}],\n",
       "  'link': 'https://ollama.com/blog/llama3.2-vision',\n",
       "  'summary': 'Llama 3.2 Vision 11B and 90B models are now available in Ollama.',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Llama 3.2 Vision 11B and 90B models are now available in Ollama.'},\n",
       "  'id': 'https://ollama.com/blog/llama3.2-vision',\n",
       "  'guidislink': False,\n",
       "  'published': 'Wed, 06 Nov 2024 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2024, tm_mon=11, tm_mday=6, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=2, tm_yday=311, tm_isdst=0)},\n",
       " {'title': 'Ollama Python library 0.4 with function calling improvements',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Ollama Python library 0.4 with function calling improvements'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/functions-as-tools'}],\n",
       "  'link': 'https://ollama.com/blog/functions-as-tools',\n",
       "  'summary': 'With Ollama Python library version 0.4, functions can now be provided as tools. The library now also has full typing support and new examples have been added.',\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'With Ollama Python library version 0.4, functions can now be provided as tools. The library now also has full typing support and new examples have been added.'},\n",
       "  'id': 'https://ollama.com/blog/functions-as-tools',\n",
       "  'guidislink': False,\n",
       "  'published': 'Mon, 25 Nov 2024 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2024, tm_mon=11, tm_mday=25, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=0, tm_yday=330, tm_isdst=0)},\n",
       " {'title': 'Structured outputs',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Structured outputs'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/structured-outputs'}],\n",
       "  'link': 'https://ollama.com/blog/structured-outputs',\n",
       "  'summary': \"Ollama now supports structured outputs making it possible to constrain a model's output to a specific format defined by a JSON schema. The Ollama Python and JavaScript libraries have been updated to support structured outputs.\",\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': \"Ollama now supports structured outputs making it possible to constrain a model's output to a specific format defined by a JSON schema. The Ollama Python and JavaScript libraries have been updated to support structured outputs.\"},\n",
       "  'id': 'https://ollama.com/blog/structured-outputs',\n",
       "  'guidislink': False,\n",
       "  'published': 'Fri, 06 Dec 2024 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2024, tm_mon=12, tm_mday=6, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=341, tm_isdst=0)},\n",
       " {'title': 'Minions: where local and cloud LLMs meet',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': 'Minions: where local and cloud LLMs meet'},\n",
       "  'links': [{'rel': 'alternate',\n",
       "    'type': 'text/html',\n",
       "    'href': 'https://ollama.com/blog/minions'}],\n",
       "  'link': 'https://ollama.com/blog/minions',\n",
       "  'summary': \"Avanika Narayan, Dan Biderman, and Sabri Eyuboglu from Christopher Ré's Stanford Hazy Research lab, along with Avner May, Scott Linderman, James Zou, have developed a way to shift a substantial portion of LLM workloads to consumer devices by having small on-device models (such as Llama 3.2 with Ollama) collaborate with larger models in the cloud (such as GPT-4o).\",\n",
       "  'summary_detail': {'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml',\n",
       "   'value': \"Avanika Narayan, Dan Biderman, and Sabri Eyuboglu from Christopher Ré's Stanford Hazy Research lab, along with Avner May, Scott Linderman, James Zou, have developed a way to shift a substantial portion of LLM workloads to consumer devices by having small on-device models (such as Llama 3.2 with Ollama) collaborate with larger models in the cloud (such as GPT-4o).\"},\n",
       "  'id': 'https://ollama.com/blog/minions',\n",
       "  'guidislink': False,\n",
       "  'published': 'Tue, 25 Feb 2025 00:00:00 +0000',\n",
       "  'published_parsed': time.struct_time(tm_year=2025, tm_mon=2, tm_mday=25, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=1, tm_yday=56, tm_isdst=0)}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if hasattr(ssl, '_create_unverified_context'):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "feed = feedparser.parse(\"https://raw.githubusercontent.com/Olshansk/rss-feeds/refs/heads/main/feeds/feed_ollama.xml\")\n",
    "\n",
    "feed.entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b40b5577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_feed(links):\n",
    "    entries = {\"Title\": [], \"Link\": [], \"Published\": [], \"Description\": []}\n",
    "    \n",
    "    try:\n",
    "        # Linklerin üzerinden geçilir\n",
    "        for link in links:\n",
    "            feed = feedparser.parse(link)\n",
    "            \n",
    "            # Feeddeki her bir girişi işler\n",
    "            for entry in feed.entries:\n",
    "                entries[\"Title\"].append(entry.get(\"title\", \"No Title\"))\n",
    "                entries[\"Link\"].append(entry.get(\"link\", \"No Link\"))\n",
    "                entries[\"Published\"].append(entry.get(\"published\", \"No Date\"))\n",
    "                entries[\"Description\"].append(entry.get(\"description\", \"No Description\"))\n",
    "                \n",
    "    except Exception as e:\n",
    "        # Hata durumunda hata mesajını yazdır\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    # DataFrame oluşturuluyor\n",
    "    df = pd.DataFrame(entries)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "75a493e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_feed(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9438a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_clean_data(df):\n",
    "    try:\n",
    "        # Regex pattern for date extraction\n",
    "        pattern = r'(\\d{2} \\w{3} \\d{4})'\n",
    "\n",
    "        # Apply regex to the 'Published' column and extract the date\n",
    "        df['date'] = df['Published'].str.extract(pattern)\n",
    "\n",
    "        # Convert the extracted date to datetime\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%d %b %Y')\n",
    "\n",
    "        # Drop the original 'Published' column\n",
    "        df.drop(columns=['Published'], inplace=True)\n",
    "\n",
    "        # Get today's date and calculate the date 7 days ago\n",
    "        today = datetime.now()\n",
    "        seven_days_ago = today - timedelta(days=7)\n",
    "\n",
    "        # Filter the rows within the last 7 days\n",
    "        df_last_seven_days = df[(df['date'] >= seven_days_ago) & (df['date'] <= today)]\n",
    "\n",
    "        # Sort by date in descending order\n",
    "        df_last_seven_days.sort_values(by='date', ascending=False, inplace=True)\n",
    "\n",
    "        # Function to clean HTML tags\n",
    "        def clean_html(text):\n",
    "            try:\n",
    "                soup = BeautifulSoup(text, \"html.parser\")\n",
    "                return soup.get_text()\n",
    "            except Exception as e:\n",
    "                print(f\"Error cleaning HTML: {e}\")\n",
    "                return text\n",
    "\n",
    "        # Apply the HTML cleaning function and shorten descriptions to 500 characters\n",
    "        df_last_seven_days['Description'] = df_last_seven_days['Description'].apply(lambda x: clean_html(x)[:500])\n",
    "\n",
    "        # Remove newline characters from the 'Description' column\n",
    "        df_last_seven_days[\"Description\"] = df_last_seven_days[\"Description\"].str.replace(\"\\n\", \"\")\n",
    "\n",
    "        return df_last_seven_days\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing the data: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame in case of error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "42f7bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last_seven_days = extract_and_clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a4f55629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Link</th>\n",
       "      <th>Description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Defending against Prompt Injection with Struct...</td>\n",
       "      <td>http://bair.berkeley.edu/blog/2025/04/11/promp...</td>\n",
       "      <td>Recent advances in Large Language Models (LLMs...</td>\n",
       "      <td>2025-04-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>National Robotics Week — Latest Physical AI Re...</td>\n",
       "      <td>https://blogs.nvidia.com/blog/national-robotic...</td>\n",
       "      <td>Check back here throughout the week to learn t...</td>\n",
       "      <td>2025-04-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Beyond CAD: How nTop Uses AI and Accelerated C...</td>\n",
       "      <td>https://blogs.nvidia.com/blog/ntop-computer-ai...</td>\n",
       "      <td>As a teenager, Bradley Rothenberg was obsessed...</td>\n",
       "      <td>2025-04-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Myth and Mystery Await: GeForce NOW Brings ‘So...</td>\n",
       "      <td>https://blogs.nvidia.com/blog/geforce-now-thur...</td>\n",
       "      <td>Get ready to explore the Deep South. South of ...</td>\n",
       "      <td>2025-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(AI)ways a Cut Above: GeForce RTX 50 Series Ac...</td>\n",
       "      <td>https://blogs.nvidia.com/blog/studio-rtx-ai-ga...</td>\n",
       "      <td>As AI-powered tools continue to evolve, NVIDIA...</td>\n",
       "      <td>2025-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NVIDIA Celebrates Partners of the Year Advanci...</td>\n",
       "      <td>https://blogs.nvidia.com/blog/nvidia-partner-n...</td>\n",
       "      <td>NVIDIA this week recognized the contributions ...</td>\n",
       "      <td>2025-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>BrowseComp: a benchmark for browsing agents</td>\n",
       "      <td>https://openai.com/index/browsecomp</td>\n",
       "      <td>BrowseComp: a benchmark for browsing agents.</td>\n",
       "      <td>2025-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Debug-gym: an environment for AI coding tools ...</td>\n",
       "      <td>https://www.microsoft.com/en-us/research/blog/...</td>\n",
       "      <td>Developers spend a lot of time debugging code....</td>\n",
       "      <td>2025-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Hopping gives this tiny robot a leg up</td>\n",
       "      <td>https://www.sciencedaily.com/releases/2025/04/...</td>\n",
       "      <td>A hopping, insect-sized robot can jump over ga...</td>\n",
       "      <td>2025-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>OpenAI Pioneers Program</td>\n",
       "      <td>https://openai.com/index/openai-pioneers-program</td>\n",
       "      <td>Advancing model performance and real world eva...</td>\n",
       "      <td>2025-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Engineers bring sign language to 'life' using ...</td>\n",
       "      <td>https://www.sciencedaily.com/releases/2025/04/...</td>\n",
       "      <td>American Sign Language (ASL) recognition syste...</td>\n",
       "      <td>2025-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3D-printed open-source robot offers accessible...</td>\n",
       "      <td>https://www.sciencedaily.com/releases/2025/04/...</td>\n",
       "      <td>FLUID, an open-source, 3D-printed robot, offer...</td>\n",
       "      <td>2025-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Research Focus: Week of April 7, 2025</td>\n",
       "      <td>https://www.microsoft.com/en-us/research/blog/...</td>\n",
       "      <td>In this issue: We introduce a new dataset desi...</td>\n",
       "      <td>2025-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>A new robotic gripper made of measuring tape i...</td>\n",
       "      <td>https://www.sciencedaily.com/releases/2025/04/...</td>\n",
       "      <td>It's a game a lot of us played as children -- ...</td>\n",
       "      <td>2025-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NVIDIA Brings Agentic AI Reasoning to Enterpri...</td>\n",
       "      <td>https://blogs.nvidia.com/blog/google-cloud-nex...</td>\n",
       "      <td>NVIDIA is collaborating with Google Cloud to b...</td>\n",
       "      <td>2025-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>‘Black Women in Artificial Intelligence’ Found...</td>\n",
       "      <td>https://blogs.nvidia.com/blog/black-women-in-a...</td>\n",
       "      <td>Necessity is the mother of invention. And some...</td>\n",
       "      <td>2025-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Repurposing Protein Folding Models for Generat...</td>\n",
       "      <td>http://bair.berkeley.edu/blog/2025/04/08/plaid/</td>\n",
       "      <td>PLAID is a multimodal generative model that si...</td>\n",
       "      <td>2025-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Tiny, soft robot flexes its potential as a lif...</td>\n",
       "      <td>https://www.sciencedaily.com/releases/2025/04/...</td>\n",
       "      <td>A tiny, soft, flexible robot that can crawl th...</td>\n",
       "      <td>2025-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Canva enables creativity with AI</td>\n",
       "      <td>https://openai.com/index/canva-cam-adams</td>\n",
       "      <td>A conversation with Cameron Adams, Chief Produ...</td>\n",
       "      <td>2025-04-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>OpenAI’s EU Economic Blueprint</td>\n",
       "      <td>https://openai.com/global-affairs/openais-eu-e...</td>\n",
       "      <td>Today, OpenAI is sharing the EU Economic Bluep...</td>\n",
       "      <td>2025-04-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0    Defending against Prompt Injection with Struct...   \n",
       "10   National Robotics Week — Latest Physical AI Re...   \n",
       "11   Beyond CAD: How nTop Uses AI and Accelerated C...   \n",
       "12   Myth and Mystery Await: GeForce NOW Brings ‘So...   \n",
       "13   (AI)ways a Cut Above: GeForce RTX 50 Series Ac...   \n",
       "14   NVIDIA Celebrates Partners of the Year Advanci...   \n",
       "109        BrowseComp: a benchmark for browsing agents   \n",
       "28   Debug-gym: an environment for AI coding tools ...   \n",
       "39              Hopping gives this tiny robot a leg up   \n",
       "110                            OpenAI Pioneers Program   \n",
       "41   Engineers bring sign language to 'life' using ...   \n",
       "40   3D-printed open-source robot offers accessible...   \n",
       "29               Research Focus: Week of April 7, 2025   \n",
       "38   A new robotic gripper made of measuring tape i...   \n",
       "16   NVIDIA Brings Agentic AI Reasoning to Enterpri...   \n",
       "15   ‘Black Women in Artificial Intelligence’ Found...   \n",
       "1    Repurposing Protein Folding Models for Generat...   \n",
       "42   Tiny, soft robot flexes its potential as a lif...   \n",
       "111                   Canva enables creativity with AI   \n",
       "112                     OpenAI’s EU Economic Blueprint   \n",
       "\n",
       "                                                  Link  \\\n",
       "0    http://bair.berkeley.edu/blog/2025/04/11/promp...   \n",
       "10   https://blogs.nvidia.com/blog/national-robotic...   \n",
       "11   https://blogs.nvidia.com/blog/ntop-computer-ai...   \n",
       "12   https://blogs.nvidia.com/blog/geforce-now-thur...   \n",
       "13   https://blogs.nvidia.com/blog/studio-rtx-ai-ga...   \n",
       "14   https://blogs.nvidia.com/blog/nvidia-partner-n...   \n",
       "109                https://openai.com/index/browsecomp   \n",
       "28   https://www.microsoft.com/en-us/research/blog/...   \n",
       "39   https://www.sciencedaily.com/releases/2025/04/...   \n",
       "110   https://openai.com/index/openai-pioneers-program   \n",
       "41   https://www.sciencedaily.com/releases/2025/04/...   \n",
       "40   https://www.sciencedaily.com/releases/2025/04/...   \n",
       "29   https://www.microsoft.com/en-us/research/blog/...   \n",
       "38   https://www.sciencedaily.com/releases/2025/04/...   \n",
       "16   https://blogs.nvidia.com/blog/google-cloud-nex...   \n",
       "15   https://blogs.nvidia.com/blog/black-women-in-a...   \n",
       "1      http://bair.berkeley.edu/blog/2025/04/08/plaid/   \n",
       "42   https://www.sciencedaily.com/releases/2025/04/...   \n",
       "111           https://openai.com/index/canva-cam-adams   \n",
       "112  https://openai.com/global-affairs/openais-eu-e...   \n",
       "\n",
       "                                           Description       date  \n",
       "0    Recent advances in Large Language Models (LLMs... 2025-04-11  \n",
       "10   Check back here throughout the week to learn t... 2025-04-11  \n",
       "11   As a teenager, Bradley Rothenberg was obsessed... 2025-04-11  \n",
       "12   Get ready to explore the Deep South. South of ... 2025-04-10  \n",
       "13   As AI-powered tools continue to evolve, NVIDIA... 2025-04-10  \n",
       "14   NVIDIA this week recognized the contributions ... 2025-04-10  \n",
       "109       BrowseComp: a benchmark for browsing agents. 2025-04-10  \n",
       "28   Developers spend a lot of time debugging code.... 2025-04-10  \n",
       "39   A hopping, insect-sized robot can jump over ga... 2025-04-09  \n",
       "110  Advancing model performance and real world eva... 2025-04-09  \n",
       "41   American Sign Language (ASL) recognition syste... 2025-04-09  \n",
       "40   FLUID, an open-source, 3D-printed robot, offer... 2025-04-09  \n",
       "29   In this issue: We introduce a new dataset desi... 2025-04-09  \n",
       "38   It's a game a lot of us played as children -- ... 2025-04-09  \n",
       "16   NVIDIA is collaborating with Google Cloud to b... 2025-04-09  \n",
       "15   Necessity is the mother of invention. And some... 2025-04-09  \n",
       "1    PLAID is a multimodal generative model that si... 2025-04-08  \n",
       "42   A tiny, soft, flexible robot that can crawl th... 2025-04-08  \n",
       "111  A conversation with Cameron Adams, Chief Produ... 2025-04-07  \n",
       "112  Today, OpenAI is sharing the EU Economic Bluep... 2025-04-07  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_last_seven_days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9468f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "74bdcb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last_seven_days.to_excel('news.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c751fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = {\"https://bair.berkeley.edu/blog/feed.xml\": \"The Berkeley Artificial Intelligence Research Blog\",\n",
    "        \"https://feeds.feedburner.com/nvidiablog\": \"NVDIA Blog\",\n",
    "        \"https://www.microsoft.com/en-us/research/feed/\": \"Microsoft Research\",\n",
    "        \"https://www.sciencedaily.com/rss/computers_math/artificial_intelligence.xml\": \"Science Daily\",\n",
    "        \"https://research.facebook.com/feed/\" : \"META Research\",\n",
    "        \"https://openai.com/news/rss.xml\": \"OpenAI News\",\n",
    "        \"https://deepmind.google/blog/feed/basic/\" : \"Google DeepMind Blog<\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "32e212c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://bair.berkeley.edu/blog/feed.xml The Berkeley Artificial Intelligence Research Blog\n",
      "https://feeds.feedburner.com/nvidiablog NVDIA Blog\n",
      "https://www.microsoft.com/en-us/research/feed/ Microsoft Research\n",
      "https://www.sciencedaily.com/rss/computers_math/artificial_intelligence.xml Science Daily\n",
      "https://research.facebook.com/feed/ META Research\n",
      "https://openai.com/news/rss.xml OpenAI News\n",
      "https://deepmind.google/blog/feed/basic/ Google DeepMind Blog<\n"
     ]
    }
   ],
   "source": [
    "for i,j in links.items():\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6a957ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://bair.berkeley.edu/blog/feed.xml',\n",
       " 'https://feeds.feedburner.com/nvidiablog',\n",
       " 'https://www.microsoft.com/en-us/research/feed/',\n",
       " 'https://www.sciencedaily.com/rss/computers_math/artificial_intelligence.xml',\n",
       " 'https://research.facebook.com/feed/',\n",
       " 'https://openai.com/news/rss.xml',\n",
       " 'https://deepmind.google/blog/feed/basic/']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(links.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa56cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
